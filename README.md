# ğŸ“š Book Recommendation System
### NLP + LSTM + Hybrid Retrieval

---

## ğŸ” Project Overview

A large-scale book recommendation engine using Natural Language Processing (NLP). It generates semantic embeddings for over 1 million books using an LSTM-based model trained with self-supervised triplet loss, and serves recommendations via a FastAPI backend.

ğŸ“¦ **Download data, embeddings, and TF-IDF:** [Google Drive](https://drive.google.com/drive/folders/1dnTdDDlWa3BFaPdeN_fCEEDd2W-200NS?usp=sharing)

---

## ğŸ§  Core System â€” Recommendation Engine

The system uses a **hybrid retrieval strategy**:

- **TF-IDF** (lexical retrieval) â†’ candidate generation
- **LSTM semantic embeddings** â†’ ranking

This ensures topical correctness and semantic understanding.

### High-Level Architecture

```
User Query
    â†“
FastAPI (/recommend)
    â†“
Text Cleaning
    â†“
TF-IDF Candidate Selection (Top ~1000)
    â†“
LSTM Triplet Encoder (Query Embedding)
    â†“
Cosine Similarity (Embedding Re-ranking)
    â†“
Top-K Book Recommendations
```

---

## ğŸ—‚ï¸ Project Structure

```
book-recommendation/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py               # FastAPI entry point
â”‚   â”œâ”€â”€ recommender.py        # Recommendation logic (DO NOT MODIFY CARELESSLY)
â”‚   â””â”€â”€ schemas.py
â”‚
â”œâ”€â”€ artifacts/                # Production assets
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ tokenizer/
â”‚   â””â”€â”€ tfidf/
â”‚
â”œâ”€â”€ autocomplete_lstm/        # â­ Autocomplete module
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ titles.txt
â”‚   â”‚   â””â”€â”€ dataset.npz
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ extract_titles.py
â”‚   â”‚   â”œâ”€â”€ check_tokenizer.py
â”‚   â”‚   â””â”€â”€ build_dataset.py
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ train_lstm.py
â”‚   â”‚
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â””â”€â”€ predict.py
â”‚   â”‚
â”‚   â”œâ”€â”€ tokenizer/
â”‚   â”‚   â””â”€â”€ tokenizer.pkl
â”‚   â”‚
â”‚   â””â”€â”€ weights/
â”‚       â””â”€â”€ lstm.pth
â”‚
â”œâ”€â”€ src/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âœ¨ Autocomplete System

### Goal

Given a partial query like `"harry potter"`, the system provides:

- ğŸ” Matching book titles
- ğŸ”® Next-word predictions

### Architecture

```
User Input
    â†“
1. Exact Title Matching (Prefix Search)
    â†“
2. LSTM Prediction (Fallback)
    â†“
Output: Matching titles + Predicted continuation
```

### Implementation Details

| Component | Detail |
|-----------|--------|
| **Data** | 918,930 book titles extracted to `autocomplete_lstm/data/titles.txt` |
| **Tokenizer** | Vocabulary size 50,000, built on titles only, stored at `autocomplete_lstm/tokenizer/tokenizer.pkl` |
| **Dataset** | Sliding window (5-word input â†’ 2-word output), ~4.97M samples, stored as `dataset.npz` |
| **Model** | PyTorch LSTM: Embedding â†’ LSTM â†’ FC layer (predicts 2 words), GPU-enabled |

### Why Hybrid?

The LSTM model alone tends to predict generic phrases like `"of the"` or `"in the"` because it learns language frequency patterns rather than actual book titles. This is expected behavior for sequence models. The hybrid approach fixes this:

| Approach | Problem |
|----------|---------|
| Only LSTM | Predicts generic phrases |
| Only Matching | No intelligent completion |
| **Hybrid** âœ… | Best of both |

**Primary method â€” Exact Title Matching:** Matches user input directly against titles and returns real book names.

**Secondary method â€” LSTM Prediction:** Used as a fallback when no strong title matches are found.

---

## ğŸš€ API Reference

### Start the Server

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Start server
python -m uvicorn app.main:app

# 3. Open Swagger UI
# http://127.0.0.1:8000/docs
```

### `POST /recommend`

Returns top-K book recommendations for a given query.

### `POST /autocomplete`

**Request:**
```json
{
  "query": "harry potter"
}
```

**Response:**
```json
{
  "query": "harry potter",
  "matches": [
    "harry potter and the goblet of fire",
    "harry potter and the chamber of secrets"
  ],
  "prediction": "and the"
}
```

---

## ğŸ§ª Autocomplete Training Pipeline

If rebuilding the autocomplete system from scratch:

```bash
# Step 1: Extract titles
python autocomplete_lstm/preprocessing/extract_titles.py

# Step 2: Build dataset
python autocomplete_lstm/preprocessing/build_dataset.py

# Step 3: Train model
python autocomplete_lstm/model/train_lstm.py
```

---

## âš ï¸ Important Notes

### Do NOT Modify
- `artifacts/` â€” tokenizer, TF-IDF, and embeddings
- Do not retrain the recommendation model unless explicitly required

### Known Limitations

**Recommendation System:**
- Dataset is academically weighted
- Short queries can be vague

**Autocomplete System:**
- LSTM predictions are generic by design
- Exact title matching is the primary method

---

## ğŸ”® Future Improvements

- FAISS for fast similarity search
- Better autocomplete ranking
- Transformer-based autocomplete
- Web UI
- Cloud deployment

---

## ğŸ‘¤ Contribution Summary

**Existing System:**
- Hybrid recommendation engine (TF-IDF + LSTM embeddings)
- FastAPI deployment

**New Work:**
- Autocomplete system (titles-based)
- PyTorch LSTM model
- Dataset generation pipeline
- Hybrid autocomplete (matching + prediction)
- Swagger integration

---

## âœ… Status

| Feature | Status |
|---------|--------|
| Large-scale NLP recommendation system | âœ” Complete |
| Hybrid retrieval (TF-IDF + embeddings) | âœ” Complete |
| Autocomplete system | âœ” Complete |
| API-ready with Swagger UI | âœ” Complete |
| GPU-supported training (PyTorch) | âœ” Complete |