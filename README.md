# ğŸ“š Book Recommendation System (NLP + LSTM + Hybrid Retrieval)

## ğŸ” Project Overview

This project implements a **large-scale book recommendation engine** using **Natural Language Processing (NLP)**.  
It generates **semantic embeddings** for over **1 million books** using an **LSTM-based model trained with self-supervised triplet loss**, and serves recommendations via a **FastAPI backend**.

The system uses a **hybrid retrieval strategy**:
1. **TF-IDF (lexical retrieval)** for candidate generation  
2. **LSTM semantic embeddings** for fine-grained ranking  

This design ensures both **topical correctness** and **semantic relevance**.

---

## ğŸ§  High-Level Architecture

```
User Query
   â†“
FastAPI (/recommend)
   â†“
Text Cleaning
   â†“
TF-IDF Candidate Selection (Top ~1000)
   â†“
LSTM Triplet Encoder (Query Embedding)
   â†“
Cosine Similarity (Embedding Re-ranking)
   â†“
Top-K Book Recommendations
```

---

## ğŸ—‚ï¸ Project Structure

```
book-recommendation/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI entry point
â”‚   â”œâ”€â”€ recommender.py       # Core inference logic (DO NOT break)
â”‚   â””â”€â”€ schemas.py           # Request/response schemas
â”‚
â”œâ”€â”€ artifacts/               # CRITICAL: production assets
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ lstm_encoder_triplet.keras
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â”œâ”€â”€ book_embeddings.npy
â”‚   â”‚   â””â”€â”€ book_metadata.csv
â”‚   â”œâ”€â”€ tokenizer/
â”‚   â”‚   â””â”€â”€ tokenizer.pkl
â”‚   â””â”€â”€ tfidf/
â”‚       â”œâ”€â”€ vectorizer.pkl
â”‚       â””â”€â”€ tfidf_matrix.pkl
â”‚
â”œâ”€â”€ src/                     # Training & preprocessing scripts
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ model/
â”‚   â””â”€â”€ inference/
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ—ï¸ What Has Already Been Done (IMPORTANT)

### âœ… Data
- Used Goodreads dataset (only CSVs with descriptions)
- Cleaned, normalized, and filtered descriptions
- Final dataset size: ~1.04M books

### âœ… Tokenization
- Keras tokenizer with:
  - `max_vocab_size = 50,000`
  - `max_sequence_length = 300`
- Tokenizer is **frozen** and reused everywhere

### âœ… Model
- **LSTM-based encoder**
- Trained using **self-supervised triplet loss**
- Objective:
  - Pull embeddings of the same book together
  - Push embeddings of different books apart
- Output: **128-dimensional semantic embeddings**

### âœ… Embeddings
- All books encoded offline
- Stored as:
  ```
  artifacts/embeddings/book_embeddings.npy
  ```
- Embeddings are **L2-normalized**

### âœ… Retrieval Strategy
- **Hybrid approach** (very important):
  - TF-IDF â†’ candidate filtering
  - LSTM embeddings â†’ semantic ranking
- Pure embedding search was tested and found inferior without lexical grounding

### âœ… Deployment
- Served via **FastAPI**
- Single endpoint:
  ```
  POST /recommend
  ```
- Swagger UI available at:
  ```
  http://127.0.0.1:8000/docs
  ```

---

## ğŸš€ How to Run the API (Local)

### 1ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Start the server
```bash
python -m uvicorn app.main:app
```

### 3ï¸âƒ£ Open Swagger UI
```
http://127.0.0.1:8000/docs
```

### 4ï¸âƒ£ Example request
```json
{
  "query": "books about artificial intelligence",
  "top_k": 10
}
```

---

## âš ï¸ VERY IMPORTANT â€” DO NOT BREAK THESE

### âŒ Do NOT retrain unless you know what youâ€™re doing
- Tokenizer
- TF-IDF vectorizer
- Embeddings
- LSTM model

They are **tightly coupled**.

### âŒ Do NOT delete `artifacts/`
The API **depends on these at startup**.

### âŒ Do NOT return raw Pandas / NumPy objects from FastAPI
- Always convert to native Python types
- NaNs must be converted to `None`

(This was already fixed in `recommender.py`.)

---

## ğŸ§ª If You Need to Retrain (Advanced)

Only do this if:
- You are changing the dataset
- You are improving the model intentionally

Correct order:
1. Train LSTM with triplet loss
2. Regenerate book embeddings
3. Rebuild TF-IDF index (optional)
4. Restart API

---

## ğŸ§  Design Rationale (Why this works)

- **TF-IDF** ensures topical relevance
- **Triplet loss** gives real semantic separation
- **Offline embedding generation** makes inference fast
- **FastAPI** allows clean deployment and testing

This mirrors **industry-grade recommendation systems**.

---

## ğŸ“Œ Known Limitations (Expected)

- Dataset is academic-heavy
- Short queries can be ambiguous
- â€œRomanticâ€ may map to *romanticism* unless query is specific
- Not optimized for approximate NN (FAISS not used yet)

---

## ğŸ”® Possible Extensions

- FAISS for faster similarity search
- Language & year filters
- Query expansion
- Web UI
- Docker deployment
- Cloud hosting (Render / Railway / AWS)

---

## ğŸ‘¤ Handoff Notes

If youâ€™re continuing this project:
- Start with `app/recommender.py`
- Use Swagger UI to test
- Treat `artifacts/` as read-only unless retraining

---

## âœ… Final Status

âœ” End-to-end NLP pipeline  
âœ” Large-scale semantic embeddings  
âœ” Hybrid retrieval system  
âœ” Production-ready API  
